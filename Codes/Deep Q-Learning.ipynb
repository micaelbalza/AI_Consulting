{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e49593-e7d1-45e4-8bce-c1fa907104b0",
   "metadata": {},
   "source": [
    "# Implementação do deep Q-learning aplicado aos jogos CartPole-v1 e LunarLander-v2\r\n",
    "\n",
    "## **CartPole-v1**\n",
    "\n",
    "O objetivo do programa é treinar um agente para resolver o problema do **CartPole-v1**, onde o agente precisa aprender a equilibrar um bastão (pole) em cima de um carrinho (cart). O agente utiliza o algoritmo *Deep *Q-Learning** para maximizar a recompensa acumulada.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bea549-a2c8-440b-9c05-a556221e1d7b",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"CartPole1.png\" alt=\"Decision Tree 1\" style=\"width: 45%; margin-right: 10px;\"/>\n",
    "    <img src=\"CartPole2.png\" alt=\"Decision Tree 2\" style=\"width: 45%;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1aeba-3e41-4f18-9593-937d38bb3cb9",
   "metadata": {},
   "source": [
    "# **Etapas do Programa**\n",
    "\n",
    "# 1. **Instalação de Dependências**\n",
    "# 2. **Importação de Dependências**\n",
    "   - **Bibliotecas Utilizadas:**\n",
    "     - `gym`: Para criar e interagir com o ambiente do CartPole-v1.\n",
    "     - `numpy`: Para operações numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25054861-d480-4fb4-8537-f6c6257b836b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gym numpy tensorflow matplotlib\n",
    "!pip install Box2D\n",
    "!pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a0d52-b529-450e-9f1b-b7b3bbdb9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acc9bb-bf15-4ed4-b2ec-599cf9bd43d6",
   "metadata": {},
   "source": [
    "# 2. **Configuração do Ambiente**\n",
    "   - Cria o ambiente `CartPole-v1`:\n",
    "     ```python\n",
    "     env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "     ```\n",
    "   - O ambiente possui:\n",
    "     - Estados contínuos (posição, velocidade, ângulo do pole).\n",
    "     - Ações discretas (mover o carrinho para a esquerda ou direita). Ou seja, 3 possibilidades (parado, esquerda e direita)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c1ee6-04f0-4360-9080-8d23b4d5173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do ambiente CartPole-v1 e habilitação da renderização para visualização\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Obtém o número de variáveis que descrevem o estado do ambiente (posição, velocidade, etc.)\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "# Obtém o número de ações possíveis (esquerda ou direita)\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59ed1c-2a16-418e-89db-881f222fbc75",
   "metadata": {},
   "source": [
    "# 3. **Hiperparâmetros e criação da rede neural**\n",
    "\n",
    "Este trecho de código configura os **hiperparâmetros** e define uma **rede neural** para estimar a função $ Q(s, a) $, essencial para o treinamento do agente no contexto do Deep Q-Learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hiperparâmetros**\n",
    "\n",
    "Os hiperparâmetros controlam o comportamento do algoritmo de aprendizado e são configurados no início do programa:\n",
    "\n",
    "1. **`gamma = 0.99` (Fator de desconto):**\n",
    "   - Determina o peso das recompensas futuras.\n",
    "   - Valores próximos de 1 priorizam recompensas de longo prazo.\n",
    "\n",
    "2. **`epsilon = 1.0` (Taxa de exploração inicial):**\n",
    "   - Controla a probabilidade de o agente tomar ações aleatórias.\n",
    "   - Alta no início para incentivar a exploração do ambiente.\n",
    "\n",
    "3. **`epsilon_min = 0.01` (Taxa mínima de exploração):**\n",
    "   - Define o limite mínimo de $ \\epsilon $, evitando que o agente pare completamente de explorar.\n",
    "\n",
    "4. **`epsilon_decay = 0.995` (Taxa de decaimento de epsilon):**\n",
    "   - Reduz gradualmente $ \\epsilon $ ao longo do treinamento.\n",
    "   - Promove a transição de exploração para exploração.\n",
    "\n",
    "5. **`learning_rate = 0.001` (Taxa de aprendizado):**\n",
    "   - Controla o quão rápido a rede neural ajusta seus pesos durante o treinamento.\n",
    "\n",
    "6. **`batch_size = 64` (Tamanho do mini-batch):**\n",
    "   - Número de amostras usadas em cada passo de treinamento da rede neural.\n",
    "\n",
    "7. **`replay_buffer_size = 2000` (Tamanho do Replay Buffer):**\n",
    "   - Capacidade do buffer que armazena as experiências do agente (estado, ação, recompensa, próximo estado).\n",
    "\n",
    "8. **`episodes = 50` (Número de episódios):**\n",
    "   - Número total de rodadas de treinamento que o agente realizará.\n",
    "\n",
    "---\n",
    "\n",
    "## **Rede Neural para Estimar $ Q(s, a) $**\n",
    "\n",
    "A função `build_model` define uma rede neural que aproxima a função valor-ação \\( Q(s, a) \\). Essa rede é usada para decidir as ações do agente com base no estado atual.\n",
    "\n",
    "### **Arquitetura da Rede**\n",
    "1. **Entrada:**\n",
    "   - O tamanho da entrada é definido por `input_dim=state_size`, correspondente ao número de variáveis do estado.\n",
    "\n",
    "2. **Camadas Ocultas:**\n",
    "   - Duas camadas densas (fully connected) com 24 neurônios cada e função de ativação ReLU:\n",
    "     - **ReLU (Rectified Linear Unit):** Permite que a rede modele relações não lineares entre estado e ações.\n",
    "\n",
    "3. **Saída:**\n",
    "   - Uma camada densa com `action_size` neurônios (um para cada ação possível).\n",
    "   - Função de ativação **linear**: Retorna valores contínuos representando $ Q(s, a) $ para cada ação.\n",
    "\n",
    "4. **Compilação:**\n",
    "   - **Otimizador:** Adam, para ajustar os pesos da rede.\n",
    "   - **Função de perda:** Mean Squared Error (MSE), que mede o erro entre $ Q(s, a) $ previsto e o alvo calculado.\n",
    "\n",
    "### **Código:**\n",
    "```python\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(24, activation='relu', input_dim=state_size),  # Primeira camada oculta\n",
    "        Dense(24, activation='relu'),                       # Segunda camada oculta\n",
    "        Dense(action_size, activation='linear')             # Camada de saída\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')  # Compilação da rede\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82a292-e05d-493a-824f-d4be27ff1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "gamma = 0.99                # Fator de desconto\n",
    "epsilon = 1.0               # Taxa de exploração inicial\n",
    "epsilon_min = 0.01          # Taxa mínima de exploração\n",
    "epsilon_decay = 0.995       # Taxa de decaimento de epsilon\n",
    "learning_rate = 0.001       # Taxa de aprendizado\n",
    "batch_size = 64             # Tamanho do mini-batch\n",
    "replay_buffer_size = 2000   # Tamanho do Replay Buffer\n",
    "episodes = 50              # Número de episódios\n",
    "\n",
    "# Rede Neural para estimar Q(s, a)\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(24, activation='relu', input_dim=state_size),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99adb1-16d3-4af9-a0d6-706a1b1e4d5e",
   "metadata": {},
   "source": [
    "# 4. **Inicialização do Modelo e Replay Buffer**\n",
    "\n",
    "### **Modelos:**\n",
    "- **Rede Principal (`model`):**\n",
    "  - Estima $Q(s, a)$ com base nos estados do ambiente.\n",
    "- **Rede Alvo (`target_model`):**\n",
    "  - É usada para calcular $\\max Q(s', a')$, estabilizando o treinamento.\n",
    "  - Inicializada com os mesmos pesos da rede principal.\n",
    "  ```python\n",
    "  model = build_model()\n",
    "  target_model = build_model()\n",
    "  target_model.set_weights(model.get_weights())\n",
    "  ```\n",
    "\n",
    "### **Replay Buffer:**\n",
    "- Um buffer circular usado para armazenar experiências do agente ($s, a, r, s', done$).\n",
    "- Permite que a rede aprenda de amostras aleatórias, reduzindo a correlação entre as atualizações.\n",
    "  ```python\n",
    "  replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Escolha da Ação (Política $\\epsilon$-greedy)**\n",
    "\n",
    "A função `choose_action` implementa a política $\\epsilon$-greedy para decidir a próxima ação do agente:\n",
    "1. **Exploração (Ação Aleatória):**\n",
    "   - Com probabilidade $\\epsilon$, o agente escolhe uma ação aleatória para explorar o ambiente.\n",
    "2. **Exploração (Ação Baseada em Q-Value):**\n",
    "   - Com probabilidade $1 - \\epsilon$, o agente escolhe a ação com o maior valor $Q(s, a)$, prevista pela rede principal.\n",
    "   ```python\n",
    "   def choose_action(state):\n",
    "       if np.random.rand() <= epsilon:\n",
    "           return random.choice(range(action_size))  # Ação aleatória\n",
    "       q_values = model.predict(state[np.newaxis, :], verbose=0)\n",
    "       return np.argmax(q_values[0])  # Ação com maior valor Q\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Atualização da Rede Alvo**\n",
    "\n",
    "A rede alvo é atualizada periodicamente para refletir os pesos da rede principal. Isso reduz oscilações durante o treinamento:\n",
    "```python\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Replay Buffer para Treinamento**\n",
    "\n",
    "A função `replay` usa experiências armazenadas no Replay Buffer para treinar a rede principal.\n",
    "\n",
    "### **Etapas:**\n",
    "1. **Checar Disponibilidade de Dados:**\n",
    "   - Aguarda o Replay Buffer ter experiências suficientes para realizar o treinamento.\n",
    "   ```python\n",
    "   if len(replay_buffer) < batch_size:\n",
    "       return\n",
    "   ```\n",
    "\n",
    "2. **Amostragem Aleatória:**\n",
    "   - Seleciona um mini-batch aleatório do buffer.\n",
    "   ```python\n",
    "   mini_batch = random.sample(replay_buffer, batch_size)\n",
    "   states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
    "   ```\n",
    "\n",
    "3. **Cálculo do Valor-Alvo:**\n",
    "   - O valor-alvo é calculado com base na recompensa imediata ($R$) e no melhor $Q(s', a')$ estimado pela rede alvo.\n",
    "   ```python\n",
    "   target = rewards[i]\n",
    "   if not dones[i]:\n",
    "       target += gamma * np.max(q_values_next[i])\n",
    "   q_values[i][actions[i]] = target\n",
    "   ```\n",
    "\n",
    "4. **Treinamento do Modelo:**\n",
    "   - A rede principal é ajustada para minimizar a diferença entre o $Q(s, a)$ previsto e o valor-alvo.\n",
    "   ```python\n",
    "   model.fit(states, q_values, epochs=1, verbose=0, batch_size=batch_size)\n",
    "   ```\n",
    "\n",
    "5. **Decaimento de $\\epsilon$:**\n",
    "   - A taxa de exploração ($\\epsilon$) é reduzida gradualmente para priorizar a exploração.\n",
    "   ```python\n",
    "   if epsilon > epsilon_min:\n",
    "       epsilon *= epsilon_decay\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Resumo**\n",
    "1. **Modelos:** Inicializa a rede principal e a rede alvo para estimar $Q(s, a)$.\n",
    "2. **Replay Buffer:** Armazena experiências do agente para aprendizado off-policy.\n",
    "3. **Política $\\epsilon$-greedy:** Alterna entre explorar o ambiente e explorar as ações com maior valor $Q$.\n",
    "4. **Treinamento:** Usa experiências armazenadas para ajustar a rede principal e reduzir a perda.\n",
    "5. **Estabilização:** A rede alvo é atualizada periodicamente com os pesos da rede principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0379e7-609a-48d6-bd18-4d595a2099e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelo e Replay Buffer\n",
    "model = build_model()\n",
    "target_model = build_model()  # Rede Alvo\n",
    "target_model.set_weights(model.get_weights())  # Inicializar com os mesmos pesos\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# Função para selecionar a ação (epsilon-greedy)\n",
    "def choose_action(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.choice(range(action_size))  # Ação aleatória\n",
    "    q_values = model.predict(state[np.newaxis, :], verbose=0)\n",
    "    return np.argmax(q_values[0])  # Ação com maior valor Q\n",
    "\n",
    "# Atualizar os pesos da rede alvo\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Replay Buffer para treino\n",
    "def replay():\n",
    "    global epsilon\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return  # Esperar até que o Replay Buffer tenha tamanho suficiente\n",
    "\n",
    "    mini_batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    q_values = model.predict(states, verbose=0)\n",
    "    q_values_next = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += gamma * np.max(q_values_next[i])\n",
    "        q_values[i][actions[i]] = target\n",
    "\n",
    "    model.fit(states, q_values, epochs=1, verbose=0, batch_size=batch_size)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82219c4-0199-42d6-a87b-84f6c1c081f9",
   "metadata": {},
   "source": [
    "# 4. **Treinamento do Agente**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1a4c2-1a52-45bf-a5d4-17822f32af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(episodes):  # Loop principal que percorre todos os episódios configurados\n",
    "    state, _ = env.reset()  # Reinicia o ambiente e obtém o estado inicial\n",
    "    total_reward = 0  # Inicializa a recompensa total para o episódio\n",
    "\n",
    "    for time in range(500):  # Limita o número de interações dentro de um episódio\n",
    "        action = choose_action(state)  # Escolhe uma ação com a política epsilon-greedy\n",
    "        next_state, reward, done, _, _ = env.step(action)  # Executa a ação no ambiente\n",
    "        next_state = np.array(next_state)  # Converte o próximo estado para um array NumPy\n",
    "\n",
    "        # Armazena a experiência atual no Replay Buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state  # Atualiza o estado atual para o próximo estado\n",
    "        total_reward += reward  # Soma a recompensa ao total do episódio\n",
    "\n",
    "        if done:  # Verifica se o episódio terminou\n",
    "            print(f\"Episódio {e+1}/{episodes}, Pontuação: {total_reward}\")  # Exibe a pontuação final\n",
    "            break  # Interrompe o loop interno se o episódio terminou\n",
    "\n",
    "        replay()  # Treina a rede principal com experiências do Replay Buffer\n",
    "\n",
    "    # Atualiza a rede alvo a cada 10 episódios para estabilizar o treinamento\n",
    "    if e % 10 == 0:\n",
    "        update_target_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b37bb-3358-43c2-a0cc-d33c6d761d5a",
   "metadata": {},
   "source": [
    "# 5. **Testar o agente treinado**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03ac00-be99-4d7e-affc-f9508fcba9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testar o agente treinado\n",
    "for _ in range(5):\n",
    "    state, _ = env.reset()  # Apenas o estado inicial\n",
    "    state = np.array(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(model.predict(state[np.newaxis, :], verbose=0))\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736026a-7f9e-4bcc-a330-50e184b6069d",
   "metadata": {},
   "source": [
    "# **LunarLander-v2**\n",
    "\n",
    "O programa treina um agente para resolver o problema do **LunarLander-v2**, onde o objetivo é pousar suavemente um módulo lunar entre duas bandeiras. O agente aprende utilizando **Deep Q-Learning**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37089f80-02b7-451e-8113-d03860339c8f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"LunarLander1.png\" alt=\"Decision Tree 1\" style=\"width: 45%; margin-right: 10px;\"/>\n",
    "    <img src=\"LunarLander2.png\" alt=\"Decision Tree 2\" style=\"width: 45%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6bee9-1cd2-4dd4-9af1-7a59f88ea85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Configuração do ambiente\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 1000\n",
    "\n",
    "# Discretização do espaço de estados\n",
    "state_bins = [10] * 8  # Divisões para cada uma das 8 dimensões do estado\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bins = [\n",
    "    np.linspace(bounds[0], bounds[1], num - 1)\n",
    "    for bounds, num in zip(state_bounds, state_bins)\n",
    "]\n",
    "\n",
    "# Tabela Q\n",
    "action_size = env.action_space.n\n",
    "state_size = tuple(len(bins) + 1 for bins in state_bins)\n",
    "q_table = np.zeros(state_size + (action_size,))\n",
    "\n",
    "# Função para discretizar estados\n",
    "def discretize_state(state):\n",
    "    discretized = [\n",
    "        np.digitize(state[i], state_bins[i]) for i in range(len(state))\n",
    "    ]\n",
    "    return tuple(discretized)\n",
    "\n",
    "# Treinamento do agente\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Escolha da ação (epsilon-greedy)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(action_size)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Executa a ação\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "\n",
    "        # Atualização da Tabela Q\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        q_table[state][action] += learning_rate * (\n",
    "            reward + discount_factor * q_table[next_state][best_next_action]\n",
    "            - q_table[state][action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Atualizar epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    print(f\"Episódio {episode + 1}/{episodes}, Pontuação: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e9363-6df5-4738-84d0-95f3fa0722cb",
   "metadata": {},
   "source": [
    "### Bonus: Stacking: Empilhar múltiplos frames (ex.: 4 últimos frames) para capturar o movimento. Essa estratégia é importante para jogos dinâmicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150d4f5-ab28-459b-8d26-ebe0fb108efb",
   "metadata": {},
   "source": [
    "<img src=\"StreetFighter_1.png\" alt=\"Iris dataset\" width=\"300\"/>\n",
    "\n",
    "(https://www.youtube.com/watch?v=3SLNbON-upI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9c81f-d198-4ec0-9e65-8a4c7a3c325c",
   "metadata": {},
   "source": [
    "<img src=\"StreetFighter_2.png\" alt=\"Iris dataset\" width=\"500\"/>\n",
    "\n",
    "(https://www.youtube.com/watch?v=3SLNbON-upI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e14ce0-616d-4815-8c83-9c6df951d5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
